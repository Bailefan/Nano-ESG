{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from statsmodels.stats.inter_rater import fleiss_kappa, aggregate_raters\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_2_dataset = pd.read_json('data/evaluation_data/task_2.json', lines=True)\n",
    "task_2_dataset.rename(columns={'sentiment': 'sentiment_llm', 'aspect': 'aspect_llm', 'relevance_score': 'relevance_score_llm'}, inplace=True)\n",
    "\n",
    "users = [i['user_id'] for i in task_2_dataset.loc[0]['responses']['relevance']]\n",
    "\n",
    "questions = [\"relevance\", \"sentiment\", \"aspect\", \"relevant_aspect\", \"relevance_score\"]\n",
    "\n",
    "#Create a column for each response field\n",
    "for field in questions:\n",
    "    task_2_dataset[field] = task_2_dataset['responses'].apply(lambda x: {i['user_id']: i['value'] for i in x[field]} if field in x else [])\n",
    "\n",
    "group_all = task_2_dataset[task_2_dataset['metadata'].apply(lambda x: 'group-all' in x.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "sentiment_dict = {1: 'positive', 0: 'neutral', -1: 'negative'}\n",
    "sentiment_dict_reverse = {'positive': 1, 'neutral': 0, 'negative': -1}\n",
    "\n",
    "def find_overall_sentiment(row):\n",
    "    '''\n",
    "    We apply the rules described in the paper in order to determine the overall sentiment\n",
    "    '''\n",
    "    most_selected = row.max()\n",
    "    columns_with_max = row[row == most_selected].index.tolist()\n",
    "    \n",
    "    if most_selected == 0:\n",
    "        return []\n",
    "\n",
    "    #Rules to determine sentiment if there are multiple different answers\n",
    "    if len(columns_with_max) > 1:\n",
    "        if 'count__negative' in columns_with_max and 'count__positive' in columns_with_max:\n",
    "            #if we have same amount of pos and neg, take neutral\n",
    "            return 'neutral'\n",
    "        elif 'count__negative' in columns_with_max:\n",
    "            #if we have more negative than positive, take negative\n",
    "            return 'negative'\n",
    "        elif 'count__positive' in columns_with_max:\n",
    "            #If we have more positive than negative, take positive\n",
    "            return 'positive'\n",
    "    else:\n",
    "        #If we have only one answer, return that\n",
    "        return columns_with_max[0].split('__')[1]\n",
    "\n",
    "def fill_relevant_aspect(row):\n",
    "    '''\n",
    "    If there was only one aspect selected by annotators, they didnt additionaly select a\n",
    "    most relevant aspect - so in that case we have to fill with the selcted aspect\n",
    "    '''\n",
    "\n",
    "    if not row['aspect']:\n",
    "        #in case they misclicked and only selected the relevant aspect, otherwise it should be []\n",
    "        return row['relevant_aspect']\n",
    "\n",
    "    new_relevant_aspect_dict = row['relevant_aspect']\n",
    "    for user in users:\n",
    "        if user not in row['aspect']:\n",
    "            continue\n",
    "        if len(row['aspect'][user]) == 1:\n",
    "            if user in row['relevant_aspect']:\n",
    "                continue\n",
    "            else:\n",
    "                if new_relevant_aspect_dict:\n",
    "                    new_relevant_aspect_dict[user] = row['aspect'][user][0]\n",
    "                else:\n",
    "                    new_relevant_aspect_dict = {user: row['aspect'][user][0]}\n",
    "    return new_relevant_aspect_dict\n",
    "\n",
    "def find_overall_aspect(row, most_rel):\n",
    "    '''\n",
    "    We apply the rules described in the paper to find the overall aspect\n",
    "    '''\n",
    "    most_selected = row.max()\n",
    "    columns_with_max = row[row == most_selected].index.tolist()\n",
    "    \n",
    "    if most_selected == 0:\n",
    "        return []\n",
    "    \n",
    "    if len(columns_with_max) > 1:\n",
    "        #if theres more than one answer, look at the most_relevant aspect selected by annotators\n",
    "        if len([i for i in columns_with_max if i != 'aspcount__not_sure']) == 1:\n",
    "            return [i for i in columns_with_max if i != 'aspcount__not_sure'][0].split('__')[1]\n",
    "        elif len(list(most_rel.values())) > 1:\n",
    "            #if we have more than one most relevant aspect, take the one that is most selected\n",
    "            most_selected_2 = Counter(most_rel.values())\n",
    "            max_count = max(most_selected_2.values())\n",
    "            columns_with_max_2 = [value for value, count in most_selected_2.items() if count == max_count]\n",
    "            if 'not_sure' in columns_with_max_2:\n",
    "                if len([i for i in columns_with_max_2 if i != 'not_sure']) == 1:\n",
    "                    return [i for i in columns_with_max_2 if i != 'not_sure'][0]\n",
    "            if len(columns_with_max_2) > 1:\n",
    "                #if the two most relevant aspects are selected equally, take the first one randomly\n",
    "                return list(most_rel.values())[0]\n",
    "            else:\n",
    "                #take the most rel aspect that is most selected\n",
    "                return columns_with_max_2[0]\n",
    "        else:\n",
    "            #if theres only one most relevant aspect, return that\n",
    "            return list(most_rel.values())[0]\n",
    "    else:\n",
    "        #if theres only one answer, return that\n",
    "        return columns_with_max[0].split('__')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# irrelevant_samples are samples with at least one irrelevant label\n",
    "#Vary the cutoff to determine how many irrelevant labels are needed to consider a sample irrelevant\n",
    "cutoff_irrelevant = 5   #1 - one irrelevant is enough to consider a sample irrelevant\n",
    "irrelevant_samples = [True if len([j for j in list(i.values()) if j == 'no']) > (cutoff_irrelevant-1) else False for i in group_all['relevance'].to_list() if i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To reproduce Relevancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9879518072289156"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ratio of relevant answers - reproduce results by varying cutoff_irrelevant above\n",
    "1-len([i for i in irrelevant_samples if i])/len(irrelevant_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Sentiment and Aspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "field = 'relevance'\n",
    "answer = 'no'\n",
    "search_df = task_2_dataset.copy()\n",
    "\n",
    "#We apply some rule-based logic to determine the overlapping final sentiment from all users\n",
    "search_df['all_sentiments'] = search_df['sentiment'].apply(lambda x: sum(list(x.values()), []) if x else [])\n",
    "search_df['count__positive'] = search_df['all_sentiments'].apply(lambda x: x.count('positive'))\n",
    "search_df['count__negative'] = search_df['all_sentiments'].apply(lambda x: x.count('negative'))\n",
    "search_df['count__neutral'] = search_df['all_sentiments'].apply(lambda x: x.count('neutral'))\n",
    "search_df['count__not_sure'] = search_df['all_sentiments'].apply(lambda x: x.count('not_sure'))\n",
    "\n",
    "search_df['sentiment_user'] = search_df.apply(lambda x: find_overall_sentiment(x[['count__positive', 'count__negative', 'count__neutral', 'count__not_sure']]), axis=1)\n",
    "\n",
    "\n",
    "#There are different ways to find the overall aspect - either through majority voting of selected ones or considering most relevant one\n",
    "search_df['all_aspects'] = search_df['aspect'].apply(lambda x: sum(list(x.values()), []) if x else [])\n",
    "search_df['aspcount__environmental'] = search_df['all_aspects'].apply(lambda x: x.count('environmental'))\n",
    "search_df['aspcount__social'] = search_df['all_aspects'].apply(lambda x: x.count('social'))\n",
    "search_df['aspcount__governance'] = search_df['all_aspects'].apply(lambda x: x.count('governance'))\n",
    "search_df['aspcount__not_sure'] = search_df['all_aspects'].apply(lambda x: x.count('not_sure'))\n",
    "\n",
    "#if the user only selected one aspect and didnt select a relevant aspect, we fill the relevant aspect with the selected one\n",
    "search_df['relevant_aspect'] = search_df.apply(lambda x: fill_relevant_aspect(x), axis=1)\n",
    "search_df['aspect_user'] = search_df.apply(lambda x: find_overall_aspect(x[['aspcount__environmental', 'aspcount__social', 'aspcount__governance', 'aspcount__not_sure']], x['relevant_aspect']), axis=1)\n",
    "\n",
    "relevant_samples = [True if len([j for j in list(i.values()) if j == 'no']) == 0 else False for i in search_df['relevance'].to_list() if i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Continue only with relevant samples\n",
    "df = search_df[search_df[field].astype(bool)]\n",
    "rel_df = df.loc[relevant_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To reproduce Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interannotator Agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract all sentiment answers into their own columns\n",
    "sentiment_agreement = group_all.copy()\n",
    "for user in users:\n",
    "    sentiment_agreement['sentiment_' + user] = sentiment_agreement['sentiment'].apply(lambda x: x[user] if user in x else [])\n",
    "\n",
    "for user in users:\n",
    "    sentiment_agreement['sentiment_' + user] = sentiment_agreement['sentiment_' + user].apply(lambda x: [i for i in x if i in ['neutral', 'positive', 'negative']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_sent_dict = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "\n",
    "z = []\n",
    "for index, row in sentiment_agreement.iterrows():\n",
    "    z_iter = []\n",
    "    status=0\n",
    "    for user in users:\n",
    "        row_sentiment = [pos_sent_dict[i] for i in row['sentiment_' + user]]\n",
    "        if len(row_sentiment) > 1:\n",
    "            if 0 in row_sentiment and 2 in row_sentiment:\n",
    "                row_sentiment = 1\n",
    "            elif 0 in row_sentiment:\n",
    "                row_sentiment = 0\n",
    "            elif 2 in row_sentiment:\n",
    "                row_sentiment = 2\n",
    "        elif row_sentiment == []:\n",
    "            status=1\n",
    "            continue\n",
    "        else:\n",
    "            row_sentiment = row_sentiment[0]\n",
    "        z_iter.append(row_sentiment)\n",
    "    if status == 0:\n",
    "        z.append(z_iter)\n",
    "\n",
    "z, _ = aggregate_raters(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.8178652738717019)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fleiss_kappa(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabian.billert\\AppData\\Local\\Temp\\ipykernel_28624\\1947293634.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  rel_df['sentiment_user_int'] = rel_df['sentiment_user'].apply(lambda x: sentiment_dict_reverse[x] if x in sentiment_dict_reverse else 0)\n",
      "C:\\Users\\fabian.billert\\AppData\\Local\\Temp\\ipykernel_28624\\1947293634.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  rel_df['sentiment_llm_int'] = rel_df['sentiment_llm'].apply(lambda x: sentiment_dict_reverse[x] if x in sentiment_dict_reverse else 0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7986798679867987"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#accuracy of simplified user determined sentiment compared to the original sentiment\n",
    "\n",
    "rel_df['sentiment_user_int'] = rel_df['sentiment_user'].apply(lambda x: sentiment_dict_reverse[x] if x in sentiment_dict_reverse else 0)\n",
    "rel_df['sentiment_llm_int'] = rel_df['sentiment_llm'].apply(lambda x: sentiment_dict_reverse[x] if x in sentiment_dict_reverse else 0)\n",
    "accuracy_score(rel_df['sentiment_user_int'], rel_df['sentiment_llm_int'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For group all, find annotator agreement\n",
    "aspect_dict = {'environmental': 1, 'social': 2, 'governance': 3}\n",
    "\n",
    "#Extract all asepct answers into their own columns\n",
    "aspect_agreement = group_all.copy()\n",
    "for user in users:\n",
    "    aspect_agreement['aspect_' + user] = aspect_agreement['aspect'].apply(lambda x: x[user] if user in x else [])\n",
    "\n",
    "#Take most relevant aspect if there are multiple\n",
    "aspect_agreement['user_decided_aspect'] = [''] * len(aspect_agreement)\n",
    "for index, row in aspect_agreement.iterrows():\n",
    "    agg_aspects = []\n",
    "    for user in users:\n",
    "        if 'not_sure' in row['aspect_' + user]:\n",
    "            if len(row['aspect_' + user]) > 1:\n",
    "                row_aspect = [i for i in row['aspect_' + user] if i != 'not_sure'][0]\n",
    "            else:\n",
    "                row_aspect = []\n",
    "        elif len(row['aspect_' + user]) > 1:\n",
    "            if user in row['relevant_aspect']:\n",
    "                row_aspect = row['relevant_aspect'][user]\n",
    "            else:\n",
    "                row_aspect = row['aspect_' + user][0]\n",
    "        else:\n",
    "            row_aspect = row['aspect_' + user][0] if row['aspect_' + user] else []\n",
    "        aspect_agreement.at[index, 'aspect_' + user] = row_aspect\n",
    "        agg_aspects.append(row_aspect)\n",
    "    aspect_agreement.at[index, 'user_decided_aspect'] = agg_aspects\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "for user in users:\n",
    "    # aspect_agreement['aspect_' + user] = aspect_agreement['aspect_' + user].apply(lambda x: [aspect_dict[i] for i in x if i in aspect_dict])\n",
    "    aspect_agreement['aspect_' + user] = aspect_agreement['aspect_' + user].apply(lambda x: aspect_dict[x] if x else x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interannotator Agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = []\n",
    "for index, row in aspect_agreement.iterrows():\n",
    "    z_iter = []\n",
    "    status=0\n",
    "    for user in users:\n",
    "        row_aspect = row['aspect_' + user]\n",
    "        if row_aspect == []:\n",
    "            status=1\n",
    "            continue\n",
    "        z_iter.append(row_aspect)\n",
    "    if status == 0:\n",
    "        z.append(z_iter)\n",
    "\n",
    "z, _ = aggregate_raters(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.42705981362124895)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fleiss_kappa(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7847682119205298"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_df_no_unsure = rel_df[rel_df['aspect_user'] != 'not_sure']\n",
    "accuracy_score(rel_df_no_unsure['aspect_user'], rel_df_no_unsure['aspect_llm'].str.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To reproduce Relevance-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_irrelevant = 1   #1 - one irrelevant is enough to consider a sample irrelevant\n",
    "irrelevant_samples = [True if len([j for j in list(i.values()) if j == 'no']) > (cutoff_irrelevant-1) else False for i in task_2_dataset['relevance'].to_list() if i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "relevance_score_llm\n",
       "8    141\n",
       "7    101\n",
       "6     63\n",
       "9     50\n",
       "5      8\n",
       "4      6\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_counts = search_df[search_df[field].astype(bool)]['relevance_score_llm'].value_counts()\n",
    "all_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "relevance_score_llm\n",
       "6    33\n",
       "7    19\n",
       "8     7\n",
       "4     3\n",
       "5     3\n",
       "9     1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irrel_counts = search_df[search_df[field].astype(bool)].loc[irrelevant_samples]['relevance_score_llm'].value_counts()\n",
    "irrel_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "relevance_score_llm\n",
       "4    0.500000\n",
       "5    0.375000\n",
       "6    0.523810\n",
       "7    0.188119\n",
       "8    0.049645\n",
       "9    0.020000\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ratio of irrelevant samples (judged by annotators) wrt relevance score\n",
    "irrel_counts/all_counts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
